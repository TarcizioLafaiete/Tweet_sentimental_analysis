{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto Final de NLP - Sentiment Classification de tweets"
      ],
      "metadata": {
        "id": "xj_cPVqoCwmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste trabalho final da disciplina de Processamento de Linguagem Natural, foi proposto aos alunos a elaboração de um projeto de tema livre com o objetivo aplicar os modelos e técnicas aprendidas em algum problema real. O tópico escolhido pelo autor foi uma aplicação de sentiment classification voltada para comentários em redes sociais, para isto foi escolhido o dataset dísponivel no Hugging Face chamado tweet_eval nele possuimos uma entrada de quase 60k de frases na qual são separadas em tweets positivos, negativos e neutros."
      ],
      "metadata": {
        "id": "RyK0kevGArk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalando Dependências"
      ],
      "metadata": {
        "id": "kx1ci4dgCrB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A biblioteca datasets é fornecida pelo Hugging Face como uma interface para facilitar o acesso dos úsuarios aos seus datasets"
      ],
      "metadata": {
        "id": "VGidjinLAV3i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdNl3anwg7v9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manipulação do Dataset"
      ],
      "metadata": {
        "id": "MTvwH_zKC6C6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clase tweetDataset"
      ],
      "metadata": {
        "id": "cMHSvMpuAl2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta é a classe responsável pela manipulação dos dados. Ela realiza a tokenização dos dados e separa as entradas e labels em 3 grupos para serem usados na aplicação sendo eles: Train,Validation e Test"
      ],
      "metadata": {
        "id": "tdAuiUSqCs1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import random\n",
        "import copy\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Baixe os recursos necessários (você pode pular isso se já tiver feito o download)\n",
        "nltk.download('punkt')\n",
        "\n",
        "class tweetDataset:\n",
        "  def __init__(self,datasetName):\n",
        "    self.dataset = load_dataset('tweet_eval',datasetName)\n",
        "    self.text_list = self.dataset['train']['text'] + self.dataset['validation']['text'] + self.dataset['test']['text']\n",
        "    self.label_list = self.dataset['train']['label'] + self.dataset['validation']['label'] + self.dataset['test']['label']\n",
        "\n",
        "  def tokenizer(self,maxlen):\n",
        "      tweet_list = []\n",
        "      label = []\n",
        "      self.tokenizer = TweetTokenizer()\n",
        "\n",
        "      label = self.label_list\n",
        "      tokenized_tweets = [self.tokenizer.tokenize(tweet) for tweet in self.text_list]\n",
        "      self.new_tokenizer = Tokenizer()\n",
        "      self.new_tokenizer.fit_on_texts(tokenized_tweets)\n",
        "      sequence = self.new_tokenizer.texts_to_sequences(tokenized_tweets)\n",
        "      sequence = pad_sequences(sequence,maxlen=maxlen,padding='post')\n",
        "      vocab_size = len(self.new_tokenizer.word_index)\n",
        "      self.sequence = sequence\n",
        "      self.label = label\n",
        "      self.vocab_size = vocab_size + 1\n",
        "\n",
        "  def create_mask(self,sequence):\n",
        "\n",
        "      no_zero_nums = lambda array: len([x for x in array if x != 0])\n",
        "      mask_seq = []\n",
        "      new_sequence = copy.deepcopy(sequence)\n",
        "      for seq in new_sequence:\n",
        "        real_nums = no_zero_nums(seq)\n",
        "        pos1 = random.randint(0,real_nums - 1)\n",
        "        pos2 = random.randint(0,real_nums - 1)\n",
        "        seq[pos1] = self.vocab_size\n",
        "        seq[pos2] = self.vocab_size\n",
        "        mask_seq.append(seq)\n",
        "\n",
        "      return mask_seq\n",
        "\n",
        "\n",
        "  def tokenize_phrase(self,text:str,maxlen):\n",
        "      txt_list = [text]\n",
        "      tweet = [self.tokenizer.tokenize(text) for text in txt_list]\n",
        "      sequence = self.new_tokenizer.texts_to_sequences(tweet)\n",
        "      return pad_sequences(sequence,maxlen=maxlen,padding='post')\n",
        "\n",
        "  def split_dataset(self,train_perc=0.8,val_perc=0.1,test_perc=0.1):\n",
        "    train_limit = int(self.vocab_size * train_perc)\n",
        "    val_limit = int(self.vocab_size*val_perc  + train_limit)\n",
        "    test_limit = int(self.vocab_size*test_perc + train_limit + val_limit)\n",
        "\n",
        "    train = []\n",
        "    validation = []\n",
        "    test = []\n",
        "    train_label = []\n",
        "    val_label = []\n",
        "    test_label = []\n",
        "\n",
        "    size = len(self.sequence - 1)\n",
        "    for i in range(size):\n",
        "      if i < train_limit:\n",
        "        train.append(self.sequence[i])\n",
        "        train_label.append(self.label[i])\n",
        "      elif i >= train_limit and i < val_limit:\n",
        "        validation.append(self.sequence[i])\n",
        "        val_label.append(self.label[i])\n",
        "      else:\n",
        "        test.append(self.sequence[i])\n",
        "        test_label.append(self.label[i])\n",
        "\n",
        "    return train,train_label,validation,val_label,test,test_label,self.vocab_size"
      ],
      "metadata": {
        "id": "nVJGM4b3paun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02c4898-6a0d-464b-b573-5fb3722f4494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Captura e separação dos dados"
      ],
      "metadata": {
        "id": "Cj7uoF4UDJhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = tweetDataset('sentiment')\n",
        "tweet.tokenizer(87)\n",
        "train_tweet,train_label,val_tweet,val_label,test_tweet,test_label,vocab_size = tweet.split_dataset()\n"
      ],
      "metadata": {
        "id": "i9o_MXDoYrrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo para Classificação"
      ],
      "metadata": {
        "id": "3FONyO4TDCLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo implementado para este problema foi uma rede composta por um Transformer Encoder que tem a sua saída ligada em uma MLP que será responsável pela classificação nas classes previamente explicadas."
      ],
      "metadata": {
        "id": "4SfUdbl6DfCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classe Transformer"
      ],
      "metadata": {
        "id": "0bHs7AqmDRL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta classe foi implementada seguindo algumas especificações dísponiveis no site do Keras. Regularizações, normalizações e dropout foram implementados a medida que foi sentido necessário."
      ],
      "metadata": {
        "id": "u2mJmZPpEQD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim,layer_rate,l2_reg,rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\",kernel_regularizer=keras.regularizers.L2(l2_reg)),\n",
        "             keras.layers.BatchNormalization(),\n",
        "             keras.layers.Dropout(rate),\n",
        "             keras.layers.Dense(embed_dim,kernel_regularizer=keras.regularizers.L2(l2_reg)),\n",
        "        ])\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=layer_rate)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=layer_rate)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs,training):\n",
        "        attn_output = self.att(inputs, inputs,use_causal_mask=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zg8bkWWRGunt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classe Embedding"
      ],
      "metadata": {
        "id": "_Iiu04sWEvEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta classe também segue padrões estabelecidos pelo Keras. Tendo outros aspectos adicionados posteriormente para melhorar a rede."
      ],
      "metadata": {
        "id": "Hti44_nsE1L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim,dropout_rate,l2_reg):\n",
        "        super().__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim,embeddings_regularizer=keras.regularizers.L2(l2_reg))\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,embeddings_regularizer=keras.regularizers.L2(l2_reg))\n",
        "        self.dropout = keras.layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def call(self, x,training):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "lDyH6TE6QpPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classe tweetModel"
      ],
      "metadata": {
        "id": "o9OPQq9dGaZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de rede completo, nele recebemos uma entrada de tamanho determinado, passo pela camada de embedding e Transformer, que são ligadas em uma rede MLP em formato Funil no qual o tamanho da camada vai dimunindo de maneira continua."
      ],
      "metadata": {
        "id": "88qiIp1mGgkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tweetModel:\n",
        "  def __init__(self,embed_dim,num_heads,ff_dim,input_length,rate,vocab_size,l2_reg,layer_norm=1e-6):\n",
        "\n",
        "    # set_memory_growth(list_physical_devices('GPU')[0], True)\n",
        "    inputs = keras.layers.Input(shape=(input_length,))\n",
        "    embedding_layer = TokenAndPositionEmbedding(input_length,vocab_size + 1,embed_dim,rate,l2_reg)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim,num_heads,ff_dim,layer_norm,l2_reg,rate)\n",
        "    x = transformer_block(x)\n",
        "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = keras.layers.Dropout(rate)(x)\n",
        "    x = keras.layers.Dense(64,activation='relu')(x)\n",
        "    x = keras.layers.Dropout(rate)(x)\n",
        "    x = keras.layers.Dense(32,activation='relu')(x)\n",
        "    x = keras.layers.Dropout(rate)(x)\n",
        "    x = keras.layers.Dense(16,activation='relu')(x)\n",
        "    x = keras.layers.Dropout(rate)(x)\n",
        "    x = keras.layers.Dense(8,activation='relu')(x)\n",
        "    x = keras.layers.Dropout(rate)(x)\n",
        "    outputs = keras.layers.Dense(3,activation='sigmoid')(x)\n",
        "    self.model = keras.Model(inputs=inputs,outputs=outputs)\n",
        "\n",
        "  def compile(self):\n",
        "    self.model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "  def summary(self):\n",
        "    self.model.summary()\n",
        "\n",
        "  def fitModel(self,train,train_label,validation,batch_size,epochs):\n",
        "    self.model.fit(train,train_label,batch_size=batch_size,epochs=epochs,validation_data=validation)\n",
        "\n",
        "  def save(self,filename):\n",
        "    self.model.save(filename)\n",
        "\n",
        "  def predict(self,input):\n",
        "    return self.model.predict(input)"
      ],
      "metadata": {
        "id": "1uKawigFQ0Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compilando o modelo"
      ],
      "metadata": {
        "id": "Ue81u17uHhB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "twModel = tweetModel(12,6,32,len(train_tweet[0]),0.1,vocab_size,1e-7,1e-7)\n",
        "twModel.compile()\n",
        "twModel.summary()"
      ],
      "metadata": {
        "id": "SrsiE0LaU4cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28c6c1e-1a0c-416b-eaed-563150794e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 87)]              0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, 87, 12)            747552    \n",
            " ng_6 (TokenAndPositionEmbe                                      \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " transformer_block_6 (Trans  (None, 87, 12)            4672      \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " global_average_pooling1d_3  (None, 12)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 12)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 64)                832       \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 755827 (2.88 MB)\n",
            "Trainable params: 755763 (2.88 MB)\n",
            "Non-trainable params: 64 (256.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "5WgbeG7lHoO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twModel.fitModel(train=np.array(train_tweet),train_label=np.array(train_label),batch_size=12,epochs=3,validation=(np.array(val_tweet),np.array(val_label)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zceFXBj3InA",
        "outputId": "7392a22d-ada7-49c4-cd63-9eb2716de82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "4148/4148 [==============================] - 136s 31ms/step - loss: 0.8735 - accuracy: 0.5899 - val_loss: 0.8047 - val_accuracy: 0.6227\n",
            "Epoch 2/3\n",
            "4148/4148 [==============================] - 127s 31ms/step - loss: 0.6974 - accuracy: 0.6981 - val_loss: 0.7778 - val_accuracy: 0.6447\n",
            "Epoch 3/3\n",
            "4148/4148 [==============================] - 127s 31ms/step - loss: 0.5923 - accuracy: 0.7574 - val_loss: 0.8017 - val_accuracy: 0.6492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando modelo"
      ],
      "metadata": {
        "id": "yN_E14fSHq2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twModel.save('tweet_sentimental.h5')"
      ],
      "metadata": {
        "id": "Ho_45WdzavXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subindo modelos para Testes"
      ],
      "metadata": {
        "id": "i64q39TQRk7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b tweet_eval https://github.com/TarcizioLafaiete/Faraday.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zd6ncrgRh7n",
        "outputId": "ad6836db-0ea0-44d2-fd10-d5f3bc658316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Faraday'...\n",
            "remote: Enumerating objects: 20090, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 20090 (delta 27), reused 90 (delta 22), pack-reused 19995\u001b[K\n",
            "Receiving objects: 100% (20090/20090), 326.27 MiB | 28.10 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste e Estatisticas de predição"
      ],
      "metadata": {
        "id": "tmIkhrNhDKfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def array_to_label(array):\n",
        "  element = max(array[0])\n",
        "  return array[0].tolist().index(element)\n",
        "\n",
        "testModel = tf.keras.models.load_model('/content/Faraday/tweet_sentimental.h5',\n",
        "                                       custom_objects={'TokenAndPositionEmbedding':TokenAndPositionEmbedding,\n",
        "                                                       'TransformerBlock':TransformerBlock})\n",
        "correct_predicts = 0\n",
        "wrong_predicts = 0\n",
        "pair_labels = []\n",
        "for i in range(len(test_tweet) - 1):\n",
        "  solution = testModel.predict([test_tweet[i].tolist()])\n",
        "  predict_label = array_to_label(solution)\n",
        "  real_label = test_label[i]\n",
        "  pair_labels.append([predict_label,real_label])\n",
        "  if predict_label == real_label:\n",
        "    correct_predicts += 1\n",
        "  else:\n",
        "    wrong_predicts += 1\n"
      ],
      "metadata": {
        "id": "2RKSb0fcaYj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"wrong_predicts: \",wrong_predicts,\" correct_predict: \",correct_predicts,\" total of predicts: \",len(test_tweet) - 1);\n",
        "print(\"Accurancy: \",correct_predicts/(len(test_tweet) - 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xx459lz_7p2",
        "outputId": "3192edbf-f880-4f5d-db09-23ff7ca8d10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrong_predicts:  1410  correct_predict:  2502  total of predicts:  3912\n",
            "Accurancy:  0.6395705521472392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def diff_mean(diff_list):\n",
        "  size = len(diff_list)\n",
        "  sum = 0\n",
        "  for diff in diff_list:\n",
        "    sum += diff\n",
        "  return sum/size\n",
        "\n",
        "def calc_metrics(pair_list,label_ref):\n",
        "  t_neg = 0\n",
        "  f_neg = 0\n",
        "  t_pos = 0\n",
        "  f_pos = 0\n",
        "  diff_list = []\n",
        "  for pair in pair_list:\n",
        "    diff_list.append(abs(pair[0] - pair[1]))\n",
        "    if pair[0] == label_ref and pair[1] == label_ref :\n",
        "      t_pos += 1\n",
        "    elif pair[0] == label_ref and pair[1] != label_ref:\n",
        "      f_pos += 1\n",
        "    elif pair[0] != label_ref and pair[1] == label_ref:\n",
        "      f_neg += 1\n",
        "    else:\n",
        "      t_neg += 1\n",
        "\n",
        "  precision = t_pos/(t_pos + f_pos)\n",
        "  recall = t_pos/(t_pos + f_neg)\n",
        "  f1_score = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "\n",
        "  return {\n",
        "      'ref' : label_ref,\n",
        "      'precision' : precision,\n",
        "      'recall' : recall,\n",
        "      'f1_score': f1_score,\n",
        "      'diff_mean' : diff_mean(diff_list)\n",
        "  }\n",
        "\n",
        "def print_statitics(labels):\n",
        "  classification = { 0 : 'negative', 1: 'neutral', 2: 'positive'}\n",
        "  print(classification[labels['ref']],\"metrics: precision: \",labels['precision'],\" recall: \",labels['recall'],\n",
        "        \" f1_score: \",labels['f1_score'],\" diff_mean: \",labels['diff_mean'])"
      ],
      "metadata": {
        "id": "KXDjLL6vIjs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_label = calc_metrics(pair_labels,1)\n",
        "print_statitics(neutral_label)\n",
        "negative_label = calc_metrics(pair_labels,0)\n",
        "print_statitics(negative_label)\n",
        "positive_label = calc_metrics(pair_labels,2)\n",
        "print_statitics(positive_label)"
      ],
      "metadata": {
        "id": "HQ4diuCROFCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bd4cac-6c20-4ea4-f334-149ca7e8e937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral metrics: precision:  0.6231884057971014  recall:  0.7155126140633387  f1_score:  0.6661669165417291  diff_mean:  0.37934560327198363\n",
            "negative metrics: precision:  0.6536170212765957  recall:  0.5840304182509506  f1_score:  0.6168674698795181  diff_mean:  0.37934560327198363\n",
            "positive metrics: precision:  0.6705685618729097  recall:  0.5463215258855586  f1_score:  0.6021021021021021  diff_mean:  0.37934560327198363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste de Novas Frases"
      ],
      "metadata": {
        "id": "HY2Q_hYnDRNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Carrega modelo ja treinado\n",
        "promptModel = tf.keras.models.load_model('/content/Faraday/tweet_sentimental.h5',\n",
        "                                       custom_objects={'TokenAndPositionEmbedding':TokenAndPositionEmbedding,\n",
        "                                                       'TransformerBlock':TransformerBlock})"
      ],
      "metadata": {
        "id": "UnNMYRAwGCGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_prhase(text):\n",
        "  #Realiza nova predicao com base na entrada coloca pelo\n",
        "  token = tweet.tokenize_phrase(text,87)\n",
        "  solution = promptModel.predict([token[0].tolist()])\n",
        "  element = max(solution[0])\n",
        "  return solution[0].tolist().index(element)\n",
        "\n",
        "classification = {0 : 'negative', 1: 'neutral', 2: 'positive'}\n",
        "user_input = input(\"Publish a new tweet: \")\n",
        "label = predict_prhase(user_input)\n",
        "print(\"Your prhase was classified as\",classification[label])"
      ],
      "metadata": {
        "id": "LfBFlBiP_lzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}